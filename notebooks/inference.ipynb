{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from unstructured.staging.base import elements_from_json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"сюда\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"сюда\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.paths import get_project_path\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_db(load_path=\"./db_BAAI_bge-m3\"):\n",
    "    vector_store = FAISS.load_local(\n",
    "        load_path, HuggingFaceEmbeddings(model_name='BAAI/bge-m3'),\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(f\"Индекс загружен из директории: {load_path}\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def get_rag_response(ques, retriever):\n",
    "    \"\"\"\n",
    "    Generates an answer using RAG and OpenAI API.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(ques)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    llm = OpenAI(base_url=os.environ[\"OPENAI_API_BASE\"])\n",
    "\n",
    "    response_big = llm.chat.completions.create(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a highly intelligent assistant who helps find and explain information.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"You have access to the following contextual information: {context} Using this information, answer the following question clearly, in detail, and professionally: Question: {ques} If the information in the context is insufficient, acknowledge it honestly and suggest a logical next step. Answer in Russian.\"\n",
    "            },\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=3000,\n",
    "    )\n",
    "\n",
    "    return response_big.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def sequential_process_rag_responses(data, retriever):\n",
    "    \"\"\"\n",
    "    Process RAG responses sequentially with rate limiting.\n",
    "    \"\"\"\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing RAG responses\"):\n",
    "        try:\n",
    "            data.at[idx, 'RAG_Ответ'] = get_rag_response(row['Вопрос'], retriever)\n",
    "            time.sleep(1.1)  # Wait for 1.1 seconds to respect the rate limit\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {idx}: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_similarity(data):\n",
    "    similarities = []\n",
    "    for idx, row in data.iterrows():\n",
    "        reference = row['Правильный ответ']\n",
    "        prediction = row['RAG_Ответ']\n",
    "        embeddings = model.encode([reference, prediction], convert_to_tensor=True)\n",
    "        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "        similarities.append(similarity)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3455655/4262839051.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  load_path, HuggingFaceEmbeddings(model_name='BAAI/bge-m3'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Индекс загружен из директории: ./db_BAAI_bge-m3\n"
     ]
    }
   ],
   "source": [
    "# Загружаем БД и передаем ее в третриве\n",
    "\n",
    "db = load_vector_db()\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RAG responses:   0%|          | 0/106 [00:00<?, ?it/s]/tmp/ipykernel_3455655/4262839051.py:14: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(ques)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RAG responses:  69%|██████▉   | 73/106 [18:07<09:24, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 72: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RAG responses:  98%|█████████▊| 104/106 [26:21<00:30, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 103: Error code: 429 - {'error': {'message': 'Rate-limit error: You send more than 1 request per 1.0 second. Try later.', 'code': 429}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RAG responses: 100%|██████████| 106/106 [27:00<00:00, 15.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# получаем ответ из валидационных вопросов\n",
    "\n",
    "data = pd.read_csv(os.path.join(get_project_path(), 'data', 'Answer_Promt.csv'))[['Вопрос', 'Правильный ответ']].iloc[:5]\n",
    "\n",
    "sequential_process_rag_responses(data, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя семантическая близость: 0.71\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('BAAI/bge-m3')\n",
    "\n",
    "data['Семантическая_Близость'] = evaluate_similarity(data)\n",
    "\n",
    "average_similarity = data['Семантическая_Близость'].dropna().mean()\n",
    "print(f\"Средняя семантическая близость: {average_similarity:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
