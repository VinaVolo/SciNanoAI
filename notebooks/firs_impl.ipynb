{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils.paths import get_project_path\n",
    "\n",
    "from unstructured_ingest.v2.pipeline.pipeline import Pipeline\n",
    "from unstructured_ingest.v2.interfaces import ProcessorConfig\n",
    "from unstructured_ingest.v2.processes.connectors.local import (\n",
    "    LocalIndexerConfig,\n",
    "    LocalDownloaderConfig,\n",
    "    LocalConnectionConfig,\n",
    "    LocalUploaderConfig\n",
    ")\n",
    "\n",
    "from unstructured.staging.base import elements_from_json\n",
    "\n",
    "from unstructured_ingest.v2.processes.partitioner import PartitionerConfig\n",
    "from unstructured_ingest.v2.processes.chunker import ChunkerConfig\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_parse_data(directory_with_pdfs: str, directory_with_results: str):\n",
    "    \n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(directory_with_pdfs):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        Pipeline.from_configs(\n",
    "            context=ProcessorConfig(),\n",
    "            indexer_config=LocalIndexerConfig(input_path=pdf_file),\n",
    "            downloader_config=LocalDownloaderConfig(),\n",
    "            source_connection_config=LocalConnectionConfig(),\n",
    "            partitioner_config=PartitionerConfig(\n",
    "                partition_by_api=True,\n",
    "                api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "                partition_endpoint=os.getenv(\"UNSTRUCTURED_API_URL\"),\n",
    "                strategy=\"hi_res\",\n",
    "                additional_partition_args={\n",
    "                    \"split_pdf_page\": True,\n",
    "                    \"split_pdf_concurrency_level\": 15,\n",
    "                },\n",
    "            ),\n",
    "            uploader_config=LocalUploaderConfig(output_dir=directory_with_results)\n",
    "        ).run()\n",
    "\n",
    "directory_with_pdfs = os.path.join(get_project_path(), 'data')\n",
    "directory_with_results = os.path.join(get_project_path(), 'data', 'parsed_pages')\n",
    "\n",
    "# local_parse_data(directory_with_pdfs, directory_with_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_files(directory_path: str) -> list:\n",
    "    elements = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                elements.extend(elements_from_json(filename=file_path))\n",
    "            except IOError:\n",
    "                print(f\"Error: Could not read file {filename}.\")\n",
    "\n",
    "    return elements\n",
    "\n",
    "elements = load_processed_files(directory_with_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Use pytorch device_name: cuda\n",
      "INFO: Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for element in elements:\n",
    "    metadata = element.metadata.to_dict()\n",
    "    documents.append(Document(page_content=element.text, metadata=metadata))\n",
    "\n",
    "db = FAISS.from_documents(documents, HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\"))\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Ollama model and prompt configuration\n",
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "\n",
    "# Define the prompt template for question answering\n",
    "prompt_template = \"\"\"\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "You are an assistant for answering questions based on extracted document sections.\n",
    "Provide clear and concise answers to the question using the context below.\n",
    "Question: {question}\n",
    "Context: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    # Format extracted document sections for the context\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#Build the RAG chain using the ollama-based LLM\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "        | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It seems like we're starting with a greeting. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2PP\n",
      "2pp_for_biomedicine\n",
      "materials_for_2pp\n",
      "structure_created_by_2pp\n",
      "methods_nanostructure\n",
      "Conventional photolithography and reactive ion etching\n",
      "High energy beam processing method\n",
      "Hot embossing\n",
      "idk\n",
      "Interference lithography\n",
      "Nanoimprinting\n",
      "Thin film deposition processing technology\n",
      "ml_and_nanostructure\n",
      "2018_Generative Model for the Inverse Design of Metasurfaces.pdf\n",
      "2018_Plasmonic nanostructure design and characterization via Deep Learning.pdf\n",
      "2019_Deep learning for accelerated all-dielectric metasurface design.pdf\n",
      "2019_Optimisation of colour generation from dielectric nanostructures using reinforcement learning.pdf\n",
      "2019_Training artificial neural network for optimization of nanostructured VO2-based smart window performance.pdf\n",
      "2020_Machine-Learning-Guided Morphology Engineering of Nanoscale Metal-Organic Frameworks.pdf\n",
      "2020_Multitask deep-learning-based design of chiral plasmonic metamaterials.pdf\n",
      "2020_Nanomaterial Synthesis Insights from Machine Learning of Scientific Articles by Extracting, Structuring, and Visualizing Knowledge.pdf\n",
      "nanostructurer_and_cells\n",
      "1-s2.0-S0014482707000924-main.pdf\n",
      "1-s2.0-S0142961205010409-main.pdf\n",
      "1-s2.0-S0301462201002472-main.pdf\n",
      "11_s41596-019-0161-7.pdf\n",
      "14_1-s2.0-S095656631530155X-main.pdf\n",
      "16_Cells_react_to_nanoscale_order_and_symmetry_in_their_surroundings.pdf\n",
      "2PP-NP-applsci-10-02398.pdf\n",
      "Capture using nanotopography.pdf\n",
      "Cells_response_to_topographic_and_chemical_micropa.pdf\n",
      "Creationofnanostructuresbyinterferencelithographyformodulationofcellbehavior.pdf\n",
      "J Biomedical Materials Res - 2005 - Swan - Fabrication and evaluation of nanoporous alumina membranes for osteoblast.pdf\n",
      "Journal of Biomedical Materials Research - 2000 - Webster - Specific proteins mediate enhanced osteoblast adhesion on.pdf\n",
      "kweon-et-al-2016-dynamic-micropatterning-of-cells-on-nanostructured-surfaces-using-a-cell-friendly-photoresist.pdf\n",
      "nanotubes_and_nanopores\n",
      "NS+Au.pdf\n",
      "NS-petri dish-cells.pdf\n",
      "PDMS-NS-C2C12.pdf\n"
     ]
    }
   ],
   "source": [
    "for filename1 in os.listdir(os.path.join(get_project_path(), 'data', 'Оригинальные статьи')):\n",
    "    print(filename1)\n",
    "    for filename2 in os.listdir(os.path.join(get_project_path(), 'data', 'Оригинальные статьи', filename1)):\n",
    "        if filename2.endswith('.pdf'):\n",
    "            \n",
    "        print(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
